{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D-BS1jy1d5j9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv('My_Product_Descriptions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "VqEu1-TxeP7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename for convenience\n",
        "df = df.rename(columns={'Category': 'label'})\n",
        "\n",
        "df['text'] = df['Product Name'].str.strip() + \" â€” \" + df['Description'].str.strip()\n",
        "\n",
        "# Encode labels\n",
        "label2id = {label: idx for idx, label in enumerate(df['label'].unique())}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "df['label_id'] = df['label'].map(label2id)\n",
        "\n",
        "# Train-validation split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)"
      ],
      "metadata": {
        "id": "cx2WTTR1eE0p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "import torch # Import torch\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class ProductDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        # The 'texts' argument is already a list because we passed train_df['text'].tolist()\n",
        "        # or val_df['text'].tolist() during instantiation.\n",
        "        # So, no need to call .tolist() again.\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding='max_length',\n",
        "                                   max_length=max_length)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Assuming labels is a Pandas Series, .iloc[idx] is appropriate\n",
        "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
        "        return item\n",
        "\n",
        "# Create datasets and loaders\n",
        "# Pass the text data as a list using .tolist()\n",
        "train_dataset = ProductDataset(train_df['text'].tolist(), train_df['label_id'], tokenizer)\n",
        "val_dataset   = ProductDataset(val_df['text'].tolist(),   val_df['label_id'], tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=16)"
      ],
      "metadata": {
        "id": "r8MqzMkdekAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.optim import AdamW # Import AdamW from torch.optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "ETRz1ewbfBUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            val_loss += outputs.loss.item()\n",
        "            preds = outputs.logits.argmax(dim=-1)\n",
        "            correct += (preds == batch['labels']).sum().item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = correct / len(val_dataset)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "4jP_UJEPfBHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save\n",
        "model.save_pretrained('bert-product-classifier')\n",
        "tokenizer.save_pretrained('bert-product-classifier')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebhtsf9Wfsra",
        "outputId": "01663816-c20e-4109-b015-9c1de1ef7175"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'Coffee', 'score': 0.8371042013168335}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load for inference\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model='bert-product-classifier',\n",
        "    tokenizer='bert-product-classifier',\n",
        "    return_all_scores=False\n",
        ")\n",
        "\n",
        "# Example\n",
        "print(classifier(\"Fruity Coffee - A fruity medium roast coffee with notes of caramel\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwdoaV_Pi_u4",
        "outputId": "8cfb83bf-cebd-4984-f744-99216311bfd7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'Coffee', 'score': 0.8371042013168335}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgsEpULilbeF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}